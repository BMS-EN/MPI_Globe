{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# import keywords from the namelist\n",
    "from namelist import lib_path, prec_keys_TS, TS_path, tag_name, lonlim, latlim, fcst_keys, tssc_keys, \\\n",
    "                     filename_08Z, TS_prefix_08Z, NCEP_path_08Z, EC_path_08Z, GRAPES_path_08Z, output_name_08Z, \\\n",
    "                     filename_20Z, TS_prefix_20Z, NCEP_path_20Z, EC_path_20Z, GRAPES_path_20Z, output_name_20Z\n",
    "\n",
    "from sys import path, argv\n",
    "path.insert(0, lib_path)\n",
    "\n",
    "# local scripts\n",
    "import micpas_tool as mt\n",
    "import ensemble_tool as et\n",
    "from utility import ini_dicts, subtrack_precip_lev, subtrack_precip_lev_heavy\n",
    "\n",
    "# other modules\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_day = -3\n",
    "day0 = 0\n",
    "key = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble post-processing starts at [20210828-18:22PM] UTC\n",
      "Import all micaps files\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    The main routine of ensemble precipitation post-porcessing. \n",
    "    '''\n",
    "    if key == 20:\n",
    "        TS_prefix = TS_prefix_20Z\n",
    "        NCEP_path = NCEP_path_20Z\n",
    "        EC_path = EC_path_20Z\n",
    "        GRAPES_path = GRAPES_path_20Z\n",
    "        filename = filename_20Z\n",
    "        output_name = output_name_20Z\n",
    "    else:\n",
    "        TS_prefix = TS_prefix_08Z\n",
    "        NCEP_path = NCEP_path_08Z\n",
    "        EC_path = EC_path_08Z\n",
    "        GRAPES_path = GRAPES_path_08Z\n",
    "        filename = filename_08Z\n",
    "        output_name = output_name_08Z\n",
    "\n",
    "    # UTC time corrections & filename creation\n",
    "    date_ref = datetime.utcnow()+relativedelta(days=delta_day)\n",
    "    date_ref_delay = date_ref-relativedelta(days=1) # use yesterday's forecast\n",
    "    date_BJ = date_ref_delay+relativedelta(hours=8)\n",
    "\n",
    "    print('Ensemble post-processing starts at ['+date_ref.strftime('%Y%m%d-%H:%M%p')+'] UTC')\n",
    "    name_today = []\n",
    "\n",
    "    name_today.append(datetime.strftime(date_BJ, EC_path))\n",
    "    name_today.append(datetime.strftime(date_BJ, NCEP_path))\n",
    "    name_today.append(datetime.strftime(date_BJ, GRAPES_path))\n",
    "\n",
    "    print('Import all micaps files')\n",
    "\n",
    "    lon, lat = mt.genrate_grid(lonlim=lonlim, latlim=latlim)\n",
    "\n",
    "    # =========== import gridded data =========== #\n",
    "\n",
    "    ## Initializing dictionaries\n",
    "\n",
    "    cmpt_keys = ['EC', 'NCEP', 'GRAPES']\n",
    "\n",
    "    dict_var = {}; dict_interp = {}; dict_header = {}\n",
    "    dict_var = ini_dicts(dict_var, cmpt_keys)\n",
    "    dict_interp = ini_dicts(dict_interp, cmpt_keys)\n",
    "\n",
    "    ## Fill dictionaries with data    \n",
    "    for fcst_key in fcst_keys:\n",
    "\n",
    "        lead = np.float(fcst_key)\n",
    "\n",
    "        if lead%24 == 0:\n",
    "            subpath = '/pre24/'\n",
    "        else:\n",
    "            subpath = '/pre03/'\n",
    "\n",
    "        for i, name in enumerate(name_today):\n",
    "\n",
    "            # identify the file path\n",
    "            temp_name = name+subpath+datetime.strftime(date_BJ, filename)+fcst_key\n",
    "\n",
    "            # read from micaps (will return false if it failed)\n",
    "            temp = mt.micaps_import(temp_name)\n",
    "\n",
    "            if temp == False:\n",
    "                print(temp_name+' not found. Exit ...')\n",
    "                #return day0\n",
    "            else:\n",
    "                dict_var[cmpt_keys[i]][fcst_key] = temp[2]\n",
    "\n",
    "        # modify the input file head and use it as the output file head\n",
    "        temp[3][0] = temp[3][0].replace(\"FZMOS\", tag_name)\n",
    "        dict_header[fcst_key] = temp[3]\n",
    "\n",
    "    # Get latlon info\n",
    "    dict_latlon = {}\n",
    "    for i, name in enumerate(name_today):\n",
    "        dict_latlon[cmpt_keys[i]] = {}\n",
    "\n",
    "    for i, name in enumerate(name_today):\n",
    "        for fcst_key in fcst_keys:\n",
    "\n",
    "            lead = np.float(fcst_key)\n",
    "\n",
    "            if lead%24 == 0:\n",
    "                subpath = '/pre24/'\n",
    "            else:\n",
    "                subpath = '/pre03/'\n",
    "\n",
    "            temp_name = name+subpath+datetime.strftime(date_BJ, filename)+fcst_key\n",
    "            dict_latlon[cmpt_keys[i]][fcst_key] = mt.micaps_import(temp_name, export_data=False)\n",
    "\n",
    "    # diff_latlon = 0\n",
    "    # diff_latlon += np.sum(np.abs(dict_latlon['EC']['024'][0] - lon)) + np.sum(np.abs(dict_latlon['EC']['024'][1] - lat))\n",
    "    # diff_latlon += np.sum(np.abs(dict_latlon['NCEP']['024'][0] - lon)) + np.sum(np.abs(dict_latlon['NCEP']['024'][1] - lat))\n",
    "    # diff_latlon += np.sum(np.abs(dict_latlon['GRAPES']['024'][0] - lon)) + np.sum(np.abs(dict_latlon['GRAPES']['024'][1] - lat))\n",
    "\n",
    "    # interpolate to the output latlon if they mismatched\n",
    "\n",
    "    print('Warning: input and output coordinates mismatched. Fixing with bilinear interpolation.')\n",
    "\n",
    "    lon, lat = mt.genrate_grid(lonlim=lonlim, latlim=latlim)\n",
    "\n",
    "    for key, val in dict_var.items():\n",
    "        for fcst_key in fcst_keys:\n",
    "            dict_interp[key][fcst_key] = mt.interp2d_wraper(dict_latlon[key][fcst_key][0], dict_latlon[key][fcst_key][1], val[fcst_key], lon, lat)\n",
    "            \n",
    "            \n",
    "    print('\\tExtracting TS for {} mm events'.format(prec_keys_TS))\n",
    "\n",
    "    W = {}; W = ini_dicts(W, prec_keys_TS)\n",
    "\n",
    "    for prec_key in prec_keys_TS:\n",
    "        W[prec_key] = ini_dicts(W[prec_key], tssc_keys)\n",
    "\n",
    "    for prec_key in prec_keys_TS:\n",
    "        for tssc_key in tssc_keys:\n",
    "\n",
    "            # retreive ts files by the fcst delay\n",
    "            date_temp = date_ref - relativedelta(days=int(tssc_key)/24+1) # \"+1\" for the one-day delay of TS \n",
    "\n",
    "            # reading TS from selected files + TS moving average\n",
    "            ## The moving averaging considers 10 days backward \n",
    "            data_ma, flag_TS = et.read_ts(date_temp.strftime(TS_prefix), TS_path+prec_key+'/', lead=tssc_key)\n",
    "\n",
    "            # saving weights to the dictionary\n",
    "            # case: no TS files (flag_TS=False)\n",
    "            if np.logical_not(flag_TS):\n",
    "                print(\"TS files do not exist. Attempting one day backward\")\n",
    "\n",
    "                date_temp = date_ref - relativedelta(days=int(tssc_key)/24+2)\n",
    "                data_ma, flag_TS = et.read_ts(date_temp.strftime(TS_prefix), TS_path+prec_key+'/', lead=tssc_key)\n",
    "\n",
    "                if np.logical_not(flag_TS):\n",
    "                    print(\"TS files do not exist. Skip.\") \n",
    "\n",
    "                #return day0 # <--- exit if no TS files\n",
    "\n",
    "            # case: TS filled with NaNs (vals = 9999.0) or filled with 0\n",
    "            ## Use TS=0.5\n",
    "\n",
    "            else:\n",
    "\n",
    "                flag_nan = np.sum(np.isnan(data_ma.values[-1, 1:].astype(np.float))) >= 3\n",
    "                flag_zero = np.sum(0 == (data_ma.values[-1, 1:].astype(np.float)) ) >= 3\n",
    "\n",
    "                if flag_nan or flag_zero:\n",
    "                    print('Warning: TS filled with NaNs or zeros, use average (TS = 1/3).')\n",
    "                    for cmpt_key in cmpt_keys:\n",
    "                        W[prec_key][tssc_key][cmpt_key] = 1/3\n",
    "                    flag_heavy = False\n",
    "\n",
    "                # case: regular (good quality) weights\n",
    "                else:\n",
    "                    for cmpt_key in cmpt_keys:\n",
    "                        temp = data_ma[cmpt_key][0]\n",
    "\n",
    "                        if np.isnan(temp):\n",
    "                            temp = 0.0\n",
    "                        W[prec_key][tssc_key][cmpt_key] = temp\n",
    "\n",
    "    # Calculate ensembles\n",
    "    print('Preparing output')\n",
    "    output = {}\n",
    "\n",
    "    # get all the result at the current day\n",
    "    print('Total: '+str(len(fcst_keys)))\n",
    "\n",
    "    thres = prec_keys_TS[0]\n",
    "\n",
    "    for i in range(len(fcst_keys)):\n",
    "\n",
    "        print('Calculating '+fcst_keys[i])\n",
    "        # --------------------------------------------------- #\n",
    "        # 0, 25, 50 mm cases\n",
    "        # initialization (three sets of weights: [0, 25, 50])\n",
    "        W0 = 0.0; W25 = 0.0; W50 = 0.0\n",
    "\n",
    "        W_EC = W[thres][tssc_keys[i]]['EC']\n",
    "        W_NCEP = W[thres][tssc_keys[i]]['NCEP']\n",
    "        W_GRAPES = W[thres][tssc_keys[i]]['GRAPES']\n",
    "        \n",
    "#         #Debugging only\n",
    "#         #print('TS EC:{}; TS NCEP: {}; TS: GRAPES {}'.format(W_EC, W_NCEP, W_GRAPES))\n",
    "#         W_sum = W_EC+W_NCEP+W_GRAPES\n",
    "        \n",
    "#         if W_sum == 0 or np.isnan(W_sum):\n",
    "#             W_EC = 1/3\n",
    "#             W_NCEP = 1/3\n",
    "#             W_GRAPES = 1/3\n",
    "\n",
    "        data0_EC, data25_EC, data50_EC = subtrack_precip_lev(dict_interp['EC'][fcst_keys[i]])\n",
    "        data0_NCEP, data25_NCEP, data50_NCEP = subtrack_precip_lev(dict_interp['NCEP'][fcst_keys[i]])\n",
    "        data0_GRAPES, data25_GRAPES, data50_GRAPES = subtrack_precip_lev(dict_interp['GRAPES'][fcst_keys[i]])\n",
    "\n",
    "        precip0 = 0.5*data0_EC + 0.5*data0_NCEP\n",
    "        precip25 = (1/3)*data25_EC + (1/3)*data25_NCEP + (1/3)*data50_GRAPES\n",
    "        precip50 = (W_EC*data50_EC + W_NCEP*data50_NCEP + W_GRAPES*data50_GRAPES)/(W_EC+W_NCEP+W_GRAPES)\n",
    "\n",
    "        output[fcst_keys[i]] = precip0 + precip25 + precip50\n",
    "        \n",
    "    # Preparing MICAPS file output\n",
    "    for fcst_key in fcst_keys:\n",
    "        metadata = mt.micaps_change_header(lon.shape, dict_header[fcst_key], lonlim, latlim)\n",
    "        mt.micaps_export(datetime.strftime(date_BJ, output_name)+fcst_key+'.txt', metadata, output[fcst_key])\n",
    "\n",
    "    print('Ensemble post-processing complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExtracting TS for ['50'] mm events\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082608\n",
      " - Checking: Test_TS/50/*2608.024\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082508\n",
      " - Checking: Test_TS/50/*2508.048\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082408\n",
      " - Checking: Test_TS/50/*2408.072\n",
      " - Checking TS at 21082308\n",
      " - Checking: Test_TS/50/*2308.096\n",
      " - Checking TS at 21082208\n",
      " - Checking: Test_TS/50/*2208.120\n",
      " - Checking TS at 21082108\n",
      " - Checking: Test_TS/50/*2108.144\n",
      " - Checking TS at 21082008\n",
      " - Checking: Test_TS/50/*2008.168\n",
      " - Checking TS at 21081908\n",
      " - Checking: Test_TS/50/*1908.192\n",
      "No files found. Exit ...\n",
      "TS files do not exist. Attempting one day backward\n",
      " - Checking TS at 21081808\n",
      " - Checking: Test_TS/50/*1808.192\n",
      "No files found. Exit ...\n",
      "TS files do not exist. Skip.\n",
      " - Checking TS at 21081808\n",
      " - Checking: Test_TS/50/*1808.216\n",
      "No files found. Exit ...\n",
      "TS files do not exist. Attempting one day backward\n",
      " - Checking TS at 21081708\n",
      " - Checking: Test_TS/50/*1708.216\n",
      "No files found. Exit ...\n",
      "TS files do not exist. Skip.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing output\n",
      "Total: 30\n",
      "Calculating 003\n",
      "Calculating 006\n",
      "Calculating 009\n",
      "Calculating 012\n",
      "Calculating 015\n",
      "Calculating 018\n",
      "Calculating 021\n",
      "Calculating 024\n",
      "Calculating 027\n",
      "Calculating 030\n",
      "Calculating 033\n",
      "Calculating 036\n",
      "Calculating 039\n",
      "Calculating 042\n",
      "Calculating 045\n",
      "Calculating 048\n",
      "Calculating 051\n",
      "Calculating 054\n",
      "Calculating 057\n",
      "Calculating 060\n",
      "Calculating 063\n",
      "Calculating 066\n",
      "Calculating 069\n",
      "Calculating 072\n",
      "Calculating 096\n",
      "Calculating 120\n",
      "Calculating 144\n",
      "Calculating 168\n",
      "Calculating 192\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'EC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-82f394f8bfce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mW0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mW25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mW50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mW_EC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtssc_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mW_NCEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtssc_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NCEP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mW_GRAPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtssc_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GRAPES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'EC'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(precip0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Exporting MPI_Globe_2021082308_024.txt\n"
     ]
    }
   ],
   "source": [
    "fcst_key = '024'\n",
    "\n",
    "metadata = mt.micaps_change_header(lon.shape, dict_header[fcst_key], lonlim, latlim)\n",
    "mt.micaps_export(datetime.strftime(date_BJ, output_name)+fcst_key+'.txt', metadata, output[fcst_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'EC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-851a1277a79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW_EC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtssc_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mW_NCEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtssc_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NCEP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mW_GRAPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtssc_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GRAPES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'EC'"
     ]
    }
   ],
   "source": [
    "W_EC = W[thres][tssc_keys[i]]['EC']\n",
    "W_NCEP = W[thres][tssc_keys[i]]['NCEP']\n",
    "W_GRAPES = W[thres][tssc_keys[i]]['GRAPES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_GRAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = mt.micaps_change_header(lon.shape, dict_header['003'], lonlim, latlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Exporting MPI_Globe_2021082308_003.txt\n"
     ]
    }
   ],
   "source": [
    "mt.micaps_export(datetime.strftime(date_BJ, output_name)+'003'+'.txt', metadata, output['003'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Checking TS at 21072108\n"
     ]
    }
   ],
   "source": [
    "print(' - Checking TS at '+date)\n",
    "\n",
    "base = datetime.strptime(date, '%y%m%d%H') # <------------- The starting date of files\n",
    "\n",
    "# loop over raw files\n",
    "# Example filename: 21072108.96 for 2021-07-21 08Z 96H forecast lead time\n",
    "# the line below searches multi-day files by matching: data_path/*08.96\n",
    "\n",
    "#names = glob(data_path+'*'+date[-2:]+'.'+lead)\n",
    "\n",
    "filename = glob(data_path+'*'+date[-4:]+'.'+lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_GRAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle no file cases\n",
    "if len(filename) == 0:\n",
    "    print('No files found in '+data_path+'. Exit ...')\n",
    "    #return 0, False\n",
    "else:\n",
    "    filename = filename[0] # list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe that contains all the valid TS\n",
    "data_local = pd.DataFrame()\n",
    "\n",
    "# temp_data: the dataframe of a single file/lead time\n",
    "# encoding='windows-1252' for windows\n",
    "temp_data = pd.read_csv(filename, delim_whitespace=True, encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exceping at least five columns for non-empty files\n",
    "## Skip this file is columns miss matched\n",
    "if len(temp_data.columns) < 5:\n",
    "    print('continue')\n",
    "    #continue\n",
    "\n",
    "## Import TS\n",
    "## File head sequence: Date, Lead, tsRR, tsEc, tsT639, tsNcep, tsJap, tsOts, tsEns, tsFre, etc.\n",
    "## <----- !!! Note: The file head must have the order above\n",
    "else:\n",
    "\n",
    "    # Discard column 2-7 (tsRR-tsJap)\n",
    "    temp_data = temp_data.drop(temp_data.columns[2:7], axis=1)\n",
    "    # Discard anything after tsFre\n",
    "    temp_data = temp_data.drop(temp_data.columns[5:], axis=1)\n",
    "\n",
    "    # Rename the pandas dataframe\n",
    "    temp_data.columns = ['Date', 'lead', 'EC', 'NCEP', 'GRAPES']\n",
    "\n",
    "    # Convert str date to datetime format\n",
    "    temp_data['Date'] = datetime.strptime(temp_data['Date'][0].astype(str), '%Y%m%d%H')\n",
    "\n",
    "    # append the current lead time to the overall dataframe\n",
    "    data_local = data_local.append(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace fill-value with NaN\n",
    "data_local = data_local.replace(9999.0, nan)\n",
    "\n",
    "# Drop forecast lead time column\n",
    "keys = ['OTS', 'ENS', 'FRE']\n",
    "data_ma = data_local.copy()\n",
    "data_ma = data_ma.drop('lead', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EC</th>\n",
       "      <th>NCEP</th>\n",
       "      <th>GRAPES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-21 08:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  EC  NCEP  GRAPES\n",
       "0 2021-07-21 08:00:00 NaN   NaN     NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # data_ma: TS as a pandas frame, flag of success\n",
    "    return data_ma, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
